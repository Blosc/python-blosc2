{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing, extending and serializing with SChunks\n",
    "\n",
    "The usual way to store generic binary data in python-blosc2 is through a `SChunk` (super-chunk) object, where the data is split into chunks of the same size, which we studied in the last tutorial. We saw how to retrieve, update or append data in the form of chunks. In fact, one can work with the individual multi-byte items composing the data (and not the bytes directly), using native SChunk methods - such operations will be the subject of this tutorial. We will use NumPy arrays as data sources, but everything we're going to do would work equally well with any Python object supporting the [Buffer Protocol](https://docs.python.org/3/c-api/buffer.html).\n",
    "\n",
    "First, we create our own `SChunk` instance; this time, let's fill it with data upon creation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:17.920892Z",
     "start_time": "2025-08-05T08:43:17.547604Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import blosc2\n",
    "\n",
    "nchunks = 10\n",
    "data = np.arange(200 * 1000 * nchunks, dtype=np.int32)\n",
    "cparams = blosc2.CParams(typesize=4)\n",
    "schunk = blosc2.SChunk(chunksize=200 * 1000 * 4, data=data, cparams=cparams)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set the `typesize` correctly as the methods we are going to use will work with items (of size ``typesize``) and not with individual bytes.\n",
    "\n",
    "## Getting data from a SChunk\n",
    "\n",
    "Let's begin by retrieving the data from the whole SChunk. We could use the `decompress_chunk` method, decompressing chunk-by-chunk into a buffer, as we did in the previous tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:17.940762Z",
     "start_time": "2025-08-05T08:43:17.927098Z"
    }
   },
   "source": [
    "out = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "for i in range(nchunks):\n",
    "    schunk.decompress_chunk(i, out[200 * 1000 * i : 200 * 1000 * (i + 1)])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "However, instead of the code above, we can simply use the `__getitem__` or the `get_slice` methods, without even needing to initialise an empty buffer. Let's begin with `__getitem__`:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.066599Z",
     "start_time": "2025-08-05T08:43:18.053030Z"
    }
   },
   "source": [
    "out_slice = schunk[:]\n",
    "type(out_slice)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As you can see, the data is returned as a bytes object. If we want to get a more meaningful container instead, we can use `get_slice`. This method requires an initialised buffer into which to load the bytes, and one may pass any Python object (supporting the Buffer Protocol) as the `out` param to fill it with the data.  In this case we will use a NumPy array container."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.127248Z",
     "start_time": "2025-08-05T08:43:18.110690Z"
    }
   },
   "source": [
    "out_slice = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "schunk.get_slice(out=out_slice)\n",
    "np.array_equal(out, out_slice)\n",
    "print(out_slice[:4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the expected data indeed!\n",
    "\n",
    "## Setting (and enlarging) data in a SChunk\n",
    "\n",
    "We can also directly set an arbitrary slice of data of a `SChunk` (without having to define a chunk and using ``update_chunk`` as we saw previously). We may use the ``__setitem__`` method of the SChunk and set it equal to some source, which may be any Python object supporting the Buffer Protocol. Let's see a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.143875Z",
     "start_time": "2025-08-05T08:43:18.136154Z"
    }
   },
   "source": [
    "start = 34\n",
    "stop = 1000 * 200 * 4\n",
    "new_value = np.ones(stop - start, dtype=np.int32)\n",
    "schunk[start:stop] = new_value"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In fact `__setitem__` allows you to set a slice of the SChunk which extends past the existing data boundaries, using essentially the same syntax:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.162572Z",
     "start_time": "2025-08-05T08:43:18.155888Z"
    }
   },
   "source": [
    "schunk_nelems = 1000 * 200 * nchunks\n",
    "\n",
    "new_value = np.zeros(1000 * 200 * 2 + 53, dtype=np.int32)\n",
    "start = schunk_nelems - 123\n",
    "new_nitems = start + new_value.size\n",
    "print(f\"Original nchunks: {schunk.nchunks}\")\n",
    "schunk[start:new_nitems] = new_value\n",
    "print(f\"New nchunks: {schunk.nchunks}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original nchunks: 10\n",
      "New nchunks: 12\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "Here, `start` is less than the number of elements in `SChunk` and `new_items` is larger; `__setitem__` can update and append data at the same time, and you don't have to worry about whether you are exceeding the limits of the `SChunk` - internally, the necessary chunks are added to accommodate the new data.  We can check that the number of chunks has indeed increased."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a SChunk from/as a contiguous buffer\n",
    "\n",
    "Recall that SChunks generally store data in a non-contiguous (sparse) manner. Certain operations (e.g. data transfer) are faster if the data is stored contiguously. Thus, one may want to convert the SChunk to a contiguous, serialized buffer (aka `cframe`). The specification of a `cframe` (a contiguous compressed representation) can be seen [here](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst). Converting to a `cframe` is as simple as calling the `to_cframe` method of the SChunk:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.182423Z",
     "start_time": "2025-08-05T08:43:18.173753Z"
    }
   },
   "source": [
    "buf = schunk.to_cframe()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Likewise, since the SChunk format is useful for e.g. extending the data in an efficient way, one may wish to convert a contiguous buffer (e.g. a `cframe`) to a SChunk. This is also very easy, as you can use the `schunk_from_cframe` method of the `blosc2` module:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:18.201578Z",
     "start_time": "2025-08-05T08:43:18.198403Z"
    }
   },
   "source": [
    "schunk2 = blosc2.schunk_from_cframe(cframe=buf, copy=True)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we set the `copy` param to `True`. If you do not want to copy the buffer, be mindful that you will have to keep a reference to it until you do not want the SChunk anymore, as the buffer memory has not been freed.\n",
    "\n",
    "\n",
    "## Serializing NumPy arrays\n",
    "\n",
    "If what you want is to create a serialized, compressed version of a NumPy array, you can bypass manually creating an SChunk and serializing it using some bespoke Blosc2 functions, which are more efficient and allow one to store the array in-memory or on-disk.\n",
    "\n",
    "**In-memory**: To compress and store the array serialized in-memory you can use `pack_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:23.038386Z",
     "start_time": "2025-08-05T08:43:18.213675Z"
    }
   },
   "source": [
    "np_array = np.arange(2**30, dtype=np.int32)  # 4 GB array\n",
    "\n",
    "packed_arr2 = blosc2.pack_tensor(np_array)\n",
    "unpacked_arr2 = blosc2.unpack_tensor(packed_arr2)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ``pack_tensor`` is way faster than the deprecated `pack_array`, which also suffers from a 2 GB size limitation.\n",
    "\n",
    "**On-disk**: To compress, store and serialize a buffer on-disk you may use `save_tensor` (and then to load into memory `load_tensor`):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:43:36.122981Z",
     "start_time": "2025-08-05T08:43:23.136917Z"
    }
   },
   "source": [
    "blosc2.save_tensor(np_array, urlpath=\"ondisk_array.b2frame\", mode=\"w\")\n",
    "np_array2 = blosc2.load_tensor(\"ondisk_array.b2frame\")\n",
    "np.array_equal(np_array, np_array2)\n",
    "blosc2.remove_urlpath(\"ondisk_array.b2frame\")  # remove the files"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The `SChunk` class is one of the stars of Python-Blosc2, as should be clear from the last two tutorials, since its sparse format allows one to easily add, insert and update compressed data rapidly. Moreover, for the situations in which one needs it, one can always get a contiguous compressed representation (aka [cframe](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst)) using the `to_cframe` method, and convert back to a SChunk using `schunk_from_cframe`. Finally, we saw how to serialize NumPy arrays in-memory or on-disk using the `pack_tensor` and `save_tensor` methods, respectively.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
