{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDArray: A NDim, Compressed Data Container\n",
    "\n",
    "NDArray objects let users perform different operations with  arrays like setting, copying or slicing them. In this section, we are going to see how to create and manipulate these NDArray arrays, which possess metadata and data. The data is *chunked* and *compressed*; the metadata gives information about the data itself, as well as the chunking and compression. Chunking and compression are features which make NDArray arrays very efficient for working with large data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:30.941529Z",
     "start_time": "2025-08-04T11:50:30.382022Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import blosc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an array\n",
    "Let's start by creating a 2D array with 100M elements filled with ``arange``. We can then print out the metadata, which contains information about: the array data (such as ``shape`` and ``dtype``); and how the data is compressed and stored, such as chunk- and block-shapes (``chunks`` and ``blocks``) and compression params (``CParams``). See [here](https://www.blosc.org/python-blosc2/getting_started/overview.html) for an explanation of chunking and blocking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:33.017064Z",
     "start_time": "2025-08-04T11:50:30.968659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (10000, 10000)\n",
      "chunks  : (100, 10000)\n",
      "blocks  : (2, 10000)\n",
      "dtype   : int64\n",
      "cratio  : 319.84\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=1, use_dict=False, typesize=8,\n",
      "        : nthreads=28, blocksize=160000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (10_000, 10_000)\n",
    "array = blosc2.arange(np.prod(shape), shape=shape)\n",
    "print(array.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``cratio`` parameter tells us how effective the compression is, since it gives the ratio between the number of bytes required to store the array in uncompressed and compressed form. Here we require almost 500x less space for the compressed array! Note that all the compression and decompression parameters are set to the default, and ``chunks`` and ``blocks`` have been selected automatically - playing around with them will affect the ``cratio`` (as well as compression and decompression speed).\n",
    "\n",
    "We can also create an NDArray by compressing a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:34.139574Z",
     "start_time": "2025-08-04T11:50:33.123637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (10000, 10000)\n",
      "chunks  : (100, 10000)\n",
      "blocks  : (2, 10000)\n",
      "dtype   : float64\n",
      "cratio  : 20.41\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=1, use_dict=False, typesize=8,\n",
      "        : nthreads=28, blocksize=160000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nparray = np.linspace(0, 100, np.prod(shape), dtype=np.float64).reshape(shape)\n",
    "b2array = blosc2.asarray(nparray)\n",
    "print(b2array.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:35.127489Z",
     "start_time": "2025-08-04T11:50:34.162393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (1000000,)\n",
      "chunks  : (500000,)\n",
      "blocks  : (31250,)\n",
      "dtype   : [('f0', '<i4'), ('f1', '<f4'), ('f2', '<f8')]\n",
      "cratio  : 2.24\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=1, use_dict=False, typesize=16,\n",
      "        : nthreads=28, blocksize=500000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 1000_000\n",
    "rng = np.random.default_rng()\n",
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,))\n",
    "print(sa.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading and modifying data\n",
    "NDArray arrays cannot be read directly, since they are compressed, and so must be decompressed first (to NumPy arrays, which are stored in memory). This can be done for the full array using the ``[:]`` operator, which returns a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:35.847424Z",
     "start_time": "2025-08-04T11:50:35.152482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = array[:]  # This will decompress the full array\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However it is often not necessary (or desirable) to load the whole array into memory. We can easily read just small parts of NDArray arrays to a NumPy array, quickly, via standard indexing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:35.887326Z",
     "start_time": "2025-08-04T11:50:35.878978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got one element (of shape (10000,)) and slice of shape (4, 10000).\n"
     ]
    }
   ],
   "source": [
    "res1 = array[0]  # get first element\n",
    "res2 = array[6:10]  # get slice\n",
    "print(f\"Got one element (of shape {res1.shape}) and slice of shape {res2.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the data in the array using standard NumPy indexing too, using either NumPy or NDArray arrays as the data source.  For example, we can set the first row to zeros (using an NDArray array) and the first column to ones (using a NumPy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:36.136815Z",
     "start_time": "2025-08-04T11:50:35.910420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<blosc2.ndarray.NDArray object at 0x7f5104194b10>\n"
     ]
    }
   ],
   "source": [
    "array[0, :] = blosc2.zeros(10000, dtype=array.dtype)\n",
    "array[:, 0] = np.ones(10000, dtype=array.dtype)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``array`` is still an NDArray array. Let's check that the entries were correctly modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:36.293666Z",
     "start_time": "2025-08-04T11:50:36.163140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 0 0 ... 0 0 0]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(array[0, 0])\n",
    "print(array[0, :])\n",
    "print(array[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enlarging the array\n",
    "Existing arrays can be enlarged. This is one operation that is greatly enhanced by the chunking procedure implemented in NDArray arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:36.329167Z",
     "start_time": "2025-08-04T11:50:36.315687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], shape=(10000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.resize((10_001, 10_000))\n",
    "print(array.shape)\n",
    "array[10_000, :] = 1\n",
    "array[10_000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlarging a NumPy array requires a full copy of the data, since underlying data are stored contiguously in memory, which is very costly: new memory to hold the extended array is allocated, the old data is copied to part of the new memory, and then the new data is written to the remaining new memory.\n",
    "Enlarging is a much faster operation for NDArray arrays because data is chunked, and the chunks may be stored non-contiguously in memory, so one may simply write the necessary new chunks to some arbitrary address in memory and leave the old chunks untouched. The references to the new chunk addresses are then added in the NDArray container, which is a very quick operation.\n",
    "\n",
    "You can also shrink the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:36.347396Z",
     "start_time": "2025-08-04T11:50:36.342261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 10000)\n",
      "[       1 89990001 89990002 ... 89999997 89999998 89999999]\n"
     ]
    }
   ],
   "source": [
    "array.resize((9_000, 10_000))\n",
    "print(array.shape)\n",
    "print(array[8_999])  # This works\n",
    "# array[9_000]  # This will raise an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent data\n",
    "We can use the `save()` method to store the array on disk.  This is very useful when you are working with a large array but do not need to access it often.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:36.672394Z",
     "start_time": "2025-08-04T11:50:36.421132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 faltet blosc 2,4M ago  6 11:31 array_tutorial.b2nd\n"
     ]
    }
   ],
   "source": [
    "array.save(\"array_tutorial.b2nd\", mode=\"w\")  # , contiguous=True)\n",
    "!ls -lh array_tutorial.b2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For arrays, it is usual to use the `.b2nd` extension. Now let's open the saved array and check that the data saved correctly (decompressing first to be able to compare):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:37.986198Z",
     "start_time": "2025-08-04T11:50:36.686674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = blosc2.open(\"array_tutorial.b2nd\")\n",
    "np.all(array2[:] == array[:])  # Make sure saved array matches original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact it is possible to create a NDArray array directly on disk, specifying where it will be stored, without first creating it in memory. We may also specify the compression/decompression and other storage parameters (e.g ``chunks`` and ``blocks``). For example, a 1000x1000 array filled with the string ``\"pepe\"`` can be created like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:38.177867Z",
     "start_time": "2025-08-04T11:50:38.007759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 faltet blosc 4,0K ago  6 11:32 array1_tutorial.b2nd\n"
     ]
    }
   ],
   "source": [
    "array1 = blosc2.full(\n",
    "    (1000, 1000),\n",
    "    fill_value=b\"pepe\",\n",
    "    chunks=(100, 100),\n",
    "    blocks=(50, 50),\n",
    "    urlpath=\"array1_tutorial.b2nd\",\n",
    "    mode=\"w\",\n",
    ")\n",
    "!ls -lh array1_tutorial.b2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write direct to disk using the other constructors we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:39.866616Z",
     "start_time": "2025-08-04T11:50:38.192813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 first rows of sa: [( 1, -2.,  0.45272938) ( 0, -1., -0.13468548) (-1,  0., -0.07419887)]\n",
      "3 first rows of b2array: [[0.00000000e+00 1.00000001e-06 2.00000002e-06 ... 9.99700010e-03\n",
      "  9.99800010e-03 9.99900010e-03]\n",
      " [1.00000001e-02 1.00010001e-02 1.00020001e-02 ... 1.99970002e-02\n",
      "  1.99980002e-02 1.99990002e-02]\n",
      " [2.00000002e-02 2.00010002e-02 2.00020002e-02 ... 2.99970003e-02\n",
      "  2.99980003e-02 2.99990003e-02]]\n"
     ]
    }
   ],
   "source": [
    "it = ((-x + 1, x - 2, rng.normal()) for x in range(N))\n",
    "sa = blosc2.fromiter(it, dtype=\"i4,f4,f8\", shape=(N,), urlpath=\"sa-1M.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of sa:\", sa[:3])\n",
    "b2array = blosc2.asarray(nparray, urlpath=\"linspace_array.b2nd\", mode=\"w\")\n",
    "print(\"3 first rows of b2array:\", b2array[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete saved data, one may use the ``remove_urlpath`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:39.891079Z",
     "start_time": "2025-08-04T11:50:39.879168Z"
    }
   },
   "outputs": [],
   "source": [
    "blosc2.remove_urlpath(\"array_tutorial.b2nd\")\n",
    "blosc2.remove_urlpath(\"array1_tutorial.b2nd\")\n",
    "blosc2.remove_urlpath(\"sa-1M.b2nd\")\n",
    "blosc2.remove_urlpath(\"linspace_array.b2nd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression params\n",
    "Let's see how to copy the NDArray data whilst altering the compression parameters. This may be useful in many contexts, for example testing how changing the codec of an existing array affects the compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:40.687447Z",
     "start_time": "2025-08-04T11:50:39.903663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (9000, 10000)\n",
      "chunks  : (500, 10000)\n",
      "blocks  : (50, 10000)\n",
      "dtype   : int64\n",
      "cratio  : 70.63\n",
      "cparams : CParams(codec=<Codec.LZ4: 1>, codec_meta=0, clevel=9, use_dict=False, typesize=8,\n",
      "        : nthreads=28, blocksize=4000000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.BITSHUFFLE: 2>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cparams = blosc2.CParams(\n",
    "    codec=blosc2.Codec.LZ4,\n",
    "    clevel=9,\n",
    "    filters=[blosc2.Filter.BITSHUFFLE],\n",
    "    filters_meta=[0],\n",
    ")\n",
    "\n",
    "array2 = array.copy(chunks=(500, 10_000), blocks=(50, 10_000), cparams=cparams)\n",
    "print(array2.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T11:50:40.713435Z",
     "start_time": "2025-08-04T11:50:40.707077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type    : NDArray\n",
      "shape   : (9000, 10000)\n",
      "chunks  : (100, 10000)\n",
      "blocks  : (2, 10000)\n",
      "dtype   : int64\n",
      "cratio  : 289.53\n",
      "cparams : CParams(codec=<Codec.ZSTD: 5>, codec_meta=0, clevel=1, use_dict=False, typesize=8,\n",
      "        : nthreads=28, blocksize=160000, splitmode=<SplitMode.AUTO_SPLIT: 3>,\n",
      "        : filters=[<Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>,\n",
      "        : <Filter.NOFILTER: 0>, <Filter.NOFILTER: 0>, <Filter.SHUFFLE: 1>], filters_meta=[0, 0,\n",
      "        : 0, 0, 0, 0], tuner=<Tuner.STUNE: 0>)\n",
      "dparams : DParams(nthreads=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(array.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the compression ratio is much higher for the original array, since we have changed to a different codec that is optimised for compression speed, not compression ratio. In general there is a tradeoff between the two.\n",
    "\n",
    "#### Native Blosc2 Codecs\n",
    "Blosc2 supports many standard codecs, since there is no one-size-fits-all compression solution - one codec may be perfect for one context, but quite suboptimal in another.\n",
    "* ZLIB codec: uses the DEFLATE algorithm, is standard, and works well for images.\n",
    "* ZSTD codec: similar compression ratio to ZLIB but faster compression/decompression\n",
    "* LZ4 codec: even faster comp/decomp than ZSTD but reduced compression ratio.\n",
    " * BloscLZ: Blosc implementation of the popular LZ algorithms (good for repeated data e.g. text). Similar tradeoff to LZ4.\n",
    "\n",
    "Finally, via package extensions to Blosc2, one may access the JPEG2000 family of compression algorithms, which aim for a compromise between compression ratio and image quality; Blosc2 implements GROK (``blosc2-grok``) and OPENHTJ2K (``blosc2-openhtj2k``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now.  There are more examples in the [examples directory of the git repository](https://github.com/Blosc/python-blosc2/tree/main/examples/ndarray) for you to explore.  Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
